---
title: "kalman_filter"
author: "Qi"
date: "2023-06-04"
output: html_document
---
#Function List
$\bullet$ `kalmanSimulate`
$\bullet$ `kalman`:
$\bullet$ `kf.gp`:
$\bullet$ `kf.loglikelihood`
$\bullet$ `optim_parm`


# Simulated Data

## Simulation of state-space model.

The following code simulate the state space model of the form $$X_t = \phi X_{t-1} + V$$ $$Y_t = X_t + W$$ where $W\sim \mathcal{N}(0,\sigma_w^2)$ and $V\sim \mathcal{N}(0,\sigma_v^2)$.

```{r}
set.seed(100)
#all Sigma here are variance namely $\sigma^2$, and lower case sigma means standard deviation.

kalmanSimulate <- function(T=100,m0=0,Sigma0=1,phi=1,Sigmav=0.02,Sigmaw=0.2){
  x <- rep(NA,T)
  y<- rep(NA,T)
  
  #create the hidden states X_t
  x[1]<-rnorm(1,mean=m0,sd=sqrt(Sigma0))
  for (i in 2:T){
    x[i] <- phi*x[i-1] + rnorm(1,mean=0,sd=sqrt(Sigmav))
  }
  
  #create the observed states Y_t
  for (i in 1:T){
    y[i] <- x[i] + rnorm(1,mean=0,sd=sqrt(Sigmaw))
  }
  return (list("x"=x,"y"=y))
}
```

We now simulate a dataset with length 100.

```{r}
simulation <- kalmanSimulate()
y <- simulation$y
originalX <- simulation$x
time <- c(1:length(y))
plot(time,y,cex=0.5,col='darkgreen',pch=5,ylim=c(-3,3))
points(time,originalX,cex=0.5,col='red',pch=10)
```

The red dots are hidden states and the green dots are the observed values which involves some independent noise.

## Kalman Filter

We will first write a Kalman filter for general state-space model, then embed that function to our specific case with $\phi = \exp(-\frac{1}{\gamma})$ and $\sigma_v^2 = 1-\exp(-\frac{2}{\gamma})$.

The `Kalman` function takes inputs $\phi$, $\sigma_v^2$, $\sigma_w^2$, $m_0$ and $\sigma_0^2$, it returns a the predicted mean, predicted variance, updated mean, updated variance, smoothing mean and smoothing variance.

```{r KF}
kalman <- function(y,phi,Sigmav,Sigmaw,m0,Sigma0){
  
  T <- length(y)
  
  #initialization
  mu.p <- rep(NA,T)
  Sigma.p <- rep(NA,T)
  mu.f <- rep(NA,T)
  Sigma.f <- rep(NA,T)
  mu.s <- rep(NA,T)
  Sigma.s <- rep(NA,T)

  
  #forward recursion time1
  mu.p[1] <- m0
  Sigma.p[1] <- Sigma0
  mu.f[1] <- m0 + (y[1]-m0)*(Sigma0/(Sigma0+Sigmaw))
  Sigma.f[1] <- Sigma0-(Sigma0^2/(Sigma0+Sigmaw))

  #forward recursion time 2:T
  for (t in 2:T){
    
    #prediction
    mu.p[t] <- phi*mu.f[t-1]
    Sigma.p[t] <- phi^2 * Sigma.f[t-1] + Sigmaw
    
    #update
    deno <- Sigmaw + Sigma.p[t]
    mu.f[t] <- Sigmaw*mu.p[t]/deno + Sigma.p[t]*y[t]/deno
    Sigma.f[t] <- Sigmaw*Sigma.p[t]/deno
  }
  
  #backword recursion
  mu.s[T] <- mu.f[T]
  Sigma.s[T] <- Sigma.f[T]
  for (t in (T-1):1){
    J <- phi*Sigma.f[t]/Sigma.p[t+1]
    mu.s[t] <- mu.f[t] + J*(mu.s[t+1]-mu.p[t+1])
    Sigma.s[t] <- Sigma.f[t] + J^2*(Sigma.s[t+1]-Sigma.p[t+1])
  }
  return (list(mu.f=mu.f,Sigma.f=Sigma.f,mu.p=mu.p,Sigma.p=Sigma.p,mu.s=mu.s,Sigma.s=Sigma.s))
}
```

We first do some experiments to see if the Kalman Filter Algorithm itself makes sense:

```{r}
results.KF <- kalman(y,phi=1,Sigmav=0.02,Sigmaw=0.2,m0=0,Sigma0=1)
mu.f <- results.KF$mu.f
Sigma.f <- results.KF$Sigma.f

se.f <- sqrt(Sigma.f)

alpha=0.01
cv99 = qnorm(1-alpha/2)
CIupper.f <- mu.f + cv99*se.f
CIlower.f <- mu.f - cv99*se.f

plot(time,y,cex=0.5,col='darkgreen',pch=5,ylim=c(-2.5,2.5),main='Updated y and observed y')
points(time,mu.f,cex=0.5,col='red',pch=10)
points(time,CIupper.f,col='blue',type ='l',lty=2,lwd=1)
points(time,CIlower.f,col='blue',type ='l',lty=2,lwd=1)
legend(1,-1,legend= c('Observation','Updated','99% Upper Confidence Interval','99% Lower Confidence Interval'), col=c('darkgreen','red','blue','blue'),lty=c(1,1,2,2),cex=.3)
```

```{r}
mu.p <- results.KF$mu.p
Sigma.p <- results.KF$Sigma.p

se.p <- sqrt(Sigma.p)

alpha=0.01
cv99 = qnorm(1-alpha/2)
CIupper.p <- mu.p + cv99*se.p
CIlower.p <- mu.p - cv99*se.p

plot(time,y,cex=0.5,col='darkgreen',pch=5,ylim=c(-2.5,2.5),main='Predicted y and observed y')
points(time,mu.p,cex=0.5,col='red',pch=10)
points(time,CIupper.p,col='blue',type ='l',lty=2,lwd=1)
points(time,CIlower.p,col='blue',type ='l',lty=2,lwd=1)
legend(1,-1,legend= c('Observation','Predicted','99% Upper Confidence Interval','99% Lower Confidence Interval'), col=c('darkgreen','red','blue','blue'),lty=c(1,1,2,2),cex=.3)
```

```{r}
mu.s <- results.KF$mu.s
Sigma.s <- results.KF$Sigma.s

se.s <- sqrt(Sigma.s)

alpha=0.01
cv99 = qnorm(1-alpha/2)
CIupper.s <- mu.s + cv99*se.s
CIlower.s <- mu.s - cv99*se.s

plot(time,y,cex=0.5,col='darkgreen',pch=5,ylim=c(-2.5,2.5),main='Smoothing y and observed y')
points(time,mu.s,cex=0.5,col='red',pch=10)
points(time,CIupper.s,col='blue',type ='l',lty=2,lwd=1)
points(time,CIlower.s,col='blue',type ='l',lty=2,lwd=1)
legend(1,-1,legend= c('Observation','Smoothing','99% Upper Confidence Interval','99% Lower Confidence Interval'), col=c('darkgreen','red','blue','blue'),lty=c(1,1,2,2),cex=.3)
```

The result makes sense, that smoothing distribution is indeed 'smoother' than the predicted values and updated values. Also we notice that the confidence interval for predictions are rather wide. All the observed values are contained in the confidence interval.

We now embed the KF algorithm into our specific case, namely take $$\phi = \exp(\frac{1}{\gamma})$$ and $$\sigma_v^2 = 1-\exp(-\frac{2}{\gamma})$$

```{r}
kf.gp <- function(y,gamma,Sigmaw,m0=0,Sigma0=1){
  T=length(y)
  #update Sigmav and phi
  phi <- exp(-1/gamma)
  Sigmav <- 1-exp(-2/gamma)
  result <- kalman(y,phi=phi,Sigmav=Sigmav,Sigmaw=Sigmaw,m0=m0,Sigma0=Sigma0)
  
  return (list(mu.p=result$mu.p, Sigma.p=result$Sigma.p,
               mu.f=result$mu.f, Sigma.f=result$Sigma.f,
               mu.s=result$mu.s, Sigma.s=result$Sigma.s))
  
}

kf.loglikelihood1 <- function(y,mu.p,Sigma.p,Sigmaw,m0=0,Sigma0=1){
  T <- length(y)
  likelihood <- rep(NA,T)
  
  #at time 1
  likelihood[1] <- log(dnorm(y[1],mean=m0,sd = sqrt(Sigma0 + Sigmaw)))
  
  #time 2:T
  for (t in 2:T){
    likelihood[t] <- log(dnorm(y[t],mean=mu.p[t],sd=sqrt(Sigmaw+Sigma.p[t])))
  }
  return (sum(likelihood))
}
```

There are two functions here: $\bullet$ `kf.gp` simply applies the Kalman Filter on observed $y$, with $\sigma_v^2$ and $\phi$ as functions of hyper parameter $\gamma$, computing the predictive, updated and smoothing distributions. $\bullet$ `kf.loglikelihood` computes the loglikelihood of the observed $y$ in a recursive regression by noting $$\log(p(y_{1:T}) = p(y_1) + \sum_{t=2}^T p(y_t|y_{1:t-1})$$ and $$p(y_t|y_{1:t-1}) = \mathcal{N}(y_t;m_{t|t-1},\sigma_{t|t-1}^2 + \sigma_w^2)$$

We now simulate with $\gamma=10$ and $\sigma_w^2=0.2$ to see if our result fit.

```{r}
set.seed(1)
gamma = 10
Sigmaw = 0.2
phi = exp(-1/gamma)
Sigmav = 1-exp(-2/gamma)
T <- 100
simulation <- kalmanSimulate(T=100,phi=phi,Sigmav=Sigmav,Sigmaw=Sigmaw)
y <- simulation$y
results <- kf.gp(y=y,gamma=gamma,Sigmaw=Sigmaw)

mu.p <- results$mu.p
Sigma.p <- results$Sigma.p

se.p <- sqrt(Sigma.p)

alpha=0.01
cv99 = qnorm(1-alpha/2)
CIupper.p <- mu.p + cv99*se.p
CIlower.p <- mu.p - cv99*se.p

plot(time,y,cex=0.5,col='darkgreen',pch=5,ylim=c(-2.5,2.5),main='Predicted y and observed y')
points(time,mu.p,cex=0.5,col='red',pch=10)
points(time,CIupper.p,col='blue',type ='l',lty=2,lwd=1)
points(time,CIlower.p,col='blue',type ='l',lty=2,lwd=1)
legend(1,-1,legend= c('Observation','Predicted','99% Upper Confidence Interval','99% Lower Confidence Interval'), col=c('darkgreen','red','blue','blue'),lty=c(1,1,2,2),cex=.3)
```

We see the KF fits indeed quite well. We further plot the smoothing distributions:

```{r}
mu.s <- results$mu.s
Sigma.s <- results$Sigma.s

se.s <- sqrt(Sigma.s)

alpha=0.01
cv99 = qnorm(1-alpha/2)
CIupper.s<- mu.s + cv99*se.s
CIlower.s <- mu.s - cv99*se.s

plot(time,y,cex=0.5,col='darkgreen',pch=5,ylim=c(-2.5,2.5),main='Predicted y and observed y')
points(time,mu.s,cex=0.5,col='red',pch=10)
points(time,CIupper.s,col='blue',type ='l',lty=2,lwd=1)
points(time,CIlower.s,col='blue',type ='l',lty=2,lwd=1)
legend(1,-1,legend= c('Observation','Smoothing','99% Upper Confidence Interval','99% Lower Confidence Interval'), col=c('darkgreen','red','blue','blue'),lty=c(1,1,2,2),cex=.3)
```
This indeed has narrower confidence interval and smoother trend. All observed values are all contained in the confidence interval.

## Hyper parameter Selection

In real life we never observe the hyper parameters $\gamma$ and $\sigma_w^2$. We propose two ways for hyper parameter selection: $\bullet$ Use `optim` to optimize against $\sigma_w^2$ and $\gamma$ against the loglikehood computed using `kf.loglikelihood`.

$\bullet$ Set the partial derivatives of the loglikelihood with respect to hyper parameters to 0. We will explain that in details later.

### Optimize over Loglikelihood

```{r}
kf.loglikelihood <- function(y,gamma,Sigmaw,m0=0,Sigma0=1){
  o <- kf.gp(y=y,gamma=gamma,Sigmaw=Sigmaw,m0=m0,Sigma0=Sigma0)
  mu.p <- o$mu.p
  Sigma.p <- o$Sigma.p
  result <- kf.loglikelihood1(y=y,mu.p=mu.p,Sigma.p=Sigma.p,Sigmaw=Sigmaw,m0=m0,Sigma0=Sigma0)
  return (result)
}

optim_parm <- function(y, m0=0, Sigma0=1){
  opt_param <- optim(par = c(5,0.5), 
                     fn = function(parm) -1*kf.loglikelihood(y,parm[1],  parm[2], m0, Sigma0))
  return(list(gamma = opt_param$par[1], 
              Sigmaw=opt_param$par[2]))
}
```

We fit the first $70\%$ of data to get an MLE estimates:
```{r}
mle <- optim_parm(y[1:80])
mle$gamma
mle$Sigmaw
```

Now we check if the MLE estimates is reasonable by making predictions on the whole data set $1:100$:
```{r}
results <- kf.gp(y=y,gamma=mle$gamma,Sigmaw=mle$Sigmaw)

mu.p <- results$mu.p
Sigma.p <- results$Sigma.p

se.p <- sqrt(Sigma.p)

alpha=0.01
cv99 = qnorm(1-alpha/2)
CIupper.p <- mu.p + cv99*se.p
CIlower.p <- mu.p - cv99*se.p

plot(time,y,cex=0.5,col='darkgreen',pch=5,ylim=c(-2.5,2.5),main='Predicted y and observed y using MLE estimates')
points(time,mu.p,cex=0.5,col='red',pch=10)
points(time,CIupper.p,col='blue',type ='l',lty=2,lwd=1)
points(time,CIlower.p,col='blue',type ='l',lty=2,lwd=1)
legend(1,-1,legend= c('Observation','Predicted','99% Upper Confidence Interval','99% Lower Confidence Interval'), col=c('darkgreen','red','blue','blue'),lty=c(1,1,2,2),cex=.3)
```
It's still doing quite well.

The second method, more like a justification to our `optim`, is to compute the partial derivatives recursively and set that values to 0. The details of derivation are included in the project report, we give the final result here that how we can recursively update $\frac{\partial m_{t|t}}{\partial \theta}$ and $\frac{\partial \sigma^2_{t|t}}{\partial \theta}$ given $\frac{\partial m_{t|t-1}}{\partial \theta}$ and $\frac{\partial \sigma^2_{t|t-1}}{\partial \theta}$. And similarly update $\frac{\partial m_{t+1|t}}{\partial \theta}$ and $\frac{\partial \sigma^2_{t+1|t}}{\partial \theta}$ given $\frac{\partial m_{t|t}}{\partial \theta}$ and $\frac{\partial \sigma^2_{t|t}}{\partial \theta}$, where $\theta$ are hyper parameters.

$$ \frac{\partial m_{t|t}}{\partial \gamma}=\frac{\partial m_{t|t-1}}{\partial \gamma}\frac{\sigma_w^2}{\sigma_w^2 + \sigma_{t|t-1}^2} + (y_t-m_{t|t-1})\frac{\partial \sigma_{t|t-1}^2}{\partial \gamma}\frac{\sigma_w^2}{(\sigma_w^2 + \sigma_{t|t-1}^2)^2}$$ $$\frac{\partial \sigma_{t|t}^2}{\partial \gamma}= \frac{\sigma_w^4}{(\sigma_w^2 + \sigma_{t|t-1}^2)^2}\frac{\partial \sigma_{t|t-1}^2}{\partial \gamma}$$ $$ \frac{\partial m_{t|t}}{\partial \sigma_w^2}=frac{\partial m_{t|t-1}}{\partial \sigma_w^2} \frac{\sigma_w^2}{\sigma_w^2 + \sigma_{t|t-1}^2} + (y_t-m_{t|t-1})\frac{\frac{\partial \sigma_{t|t-1}^2}{\partial \sigma_w^2}\sigma_w^2-\sigma_{t|t-1}^2}{(\sigma_w^2+\sigma_{t|t-1}^2)^2}$$ $$\frac{\partial \sigma_{t|t}^2}{\partial \sigma_w^2}=\sigma_w^4\frac{\partial \sigma_{t|t-1}^2}{\partial \sigma_w^2}\frac{1}{(\sigma_w^2+\sigma_{t|t-1}^2)^2} + \sigma_{t|t-1}^4\frac{1}{(\sigma_w^2+\sigma_{t|t-1}^2)^2}$$ $$\frac{\partial m_{t+1|t}}{\partial \gamma} = \frac{1}{\gamma^2}\exp(-\frac{1}{\gamma})m_{t|t} + \exp(-\frac{1}{\gamma})\frac{\partial m_{t|t}}{\partial \gamma}$$ $$\frac{\partial \sigma_{t+1|t}^2}{\partial \gamma} = -\frac{2}{\gamma^2}\exp(-\frac{2}{\gamma}) + \exp(-\frac{2}{\gamma})\frac{\partial \sigma_{t|t}^2}{\partial \gamma} + \frac{2}{\gamma^2}\exp(-\frac{2}{\gamma})\sigma_{t|t}^2$$ $$ \frac{\partial m_{t+1|t}}{\partial \sigma_w^2} = \exp(-\frac{1}{\gamma})\frac{\partial m_{t|t}}{\partial \sigma_w^2}$$ $$\frac{\partial \sigma_{t+1|t}^2}{\partial \sigma_w^2} = \exp(-\frac{2}{\gamma})\frac{\partial \sigma^2_{t|t}}{\partial \sigma_w^2}$$ Finally given all these results we have the partial derivatives with respect to the log likelihood: $$\frac{\partial \log(p(y_{k+1}|y_{1:k}))}{\partial \gamma} = \frac{y_{k+1}-m_{k+1|k}}{\sigma_{k+1|k}^2+\sigma_w^2}\frac{\partial m_{k+1|k}}{\partial \gamma} -\left(\frac{1}{2(\sigma_{k+1|k}^2+\sigma_w^2)}-\frac{(y_{k+1}-m_{k+1|k})^2}{2(\sigma_{k+1|k}^2+\sigma_w^2)^2}\right)\frac{\partial \sigma_{k+1|k}^2}{\partial \gamma}$$ and $$\frac{\partial \log(p(y_{k+1}|y_{1:k}))}{\partial \sigma_w^2} = \frac{y_{k+1}-m_{k+1|k}}{\sigma_{k+1|k}^2+\sigma_w^2}\frac{\partial m_{k+1|k}}{\partial \sigma_w^2} -\frac{1+\frac{\partial \sigma_{k+1|k}^2}{\partial \sigma_w^2}}{2(\sigma_{k+1|k}^2+\sigma_w^2)}+\frac{(y_{k+1}-m_{k+1|k})^2}{2(\sigma_{k+1|k}^2+\sigma_w^2)^2}(1+\frac{\partial \sigma_{k+1|k}^2}{\partial \sigma_w^2})$$.


```{r}
partialDerivatives <- function(y,gamma,Sigmaw,m0=0,Sigma0=1){
  T <- length(y)
  o <- kf.gp(y=y,gamma=gamma,Sigmaw=Sigmaw,m0=m0,Sigma0=Sigma0)
  mu.p <- o$mu.p
  Sigma.p <- o$Sigma.p
  mu.f <- o$mu.f
  Sigma.f <- o$Sigma.f
  partial.f.mg <- rep(NA,T)
  partial.f.sg <- rep(NA,T)
  partial.f.ms <- rep(NA,T)
  partial.f.ss <- rep(NA,T)
  partial.p.mg <- rep(NA,T)
  partial.p.sg <- rep(NA,T)
  partial.p.ms <- rep(NA,T)
  partial.p.ss <- rep(NA,T)
  partial.y.g <- rep(NA,T)
  partial.y.s <- rep(NA,T)
  
  #time 1
  partial.f.mg[1] <- 0
  partial.f.sg[1] <- 0
  partial.p.ms[1] <- 0
  partial.p.ss[1] <- 0
  partial.p.mg[1] <- 0
  partial.p.sg[1] <- 0
  partial.f.ms[1] <- -Sigma0/((Sigma0+Sigmaw)^2)*(y[1]-m0)
  partial.f.ss[1] <- Sigma0^2/((Sigma0+Sigmaw)^2)
  partial.y.g[1] <- 0
  partial.y.s[1] <- 1/(2*(Sigma0+Sigmaw))*((y[1]-m0)^2/(Sigma0+Sigmaw)-1)
  
  #time 2:T
  for (t in 2:T){
    
    #prediction
    partial.p.mg[t] <- 1/(gamma^2)*exp(-1/gamma)*mu.f[t-1] + exp(-1/gamma)*partial.f.mg[t-1]
    partial.p.sg[t] <- -2/(gamma^2)*exp(-2/gamma) + exp(-2/gamma)*partial.f.sg[t-1] + 2/(gamma^2)*exp(-2/gamma)*Sigma.f[t-1]
    partial.p.ms[t] <- exp(-1/gamma)*partial.f.ms[t-1]
    partial.p.ss[t] <- exp(-2/gamma)*partial.f.ss[t-1]
    
    #update
    partial.f.mg[t]<- partial.p.mg[t]*Sigmaw/(Sigmaw+Sigma.p[t]) + (y[t]-mu.p[t])*partial.p.sg[t]*Sigmaw/((Sigmaw+Sigma.p[t])^2)
    partial.f.sg[t]<- Sigmaw^2/((Sigmaw+Sigma.p[t])^2) * partial.p.sg[t]
    partial.f.ms[t]<- partial.p.ms[t]*Sigmaw/(Sigmaw+Sigma.p[t]) + (y[t]-mu.p[t])*(partial.p.ss[t]*Sigmaw-Sigma.p[t])/((Sigmaw+Sigma.p[t])^2)
    partial.f.ss[t]<- Sigmaw^2*partial.p.ss[t]/((Sigmaw+Sigma.p[t])^2) + Sigma.p[t]^2/((Sigmaw+Sigma.p[t])^2)
    
    #get the derivatives with respect to log p(y)
    denom <- Sigmaw + Sigma.p[t]
    partial.y.g[t] <- (y[t]-mu.p[t])/denom*partial.p.mg[t] -(1/(2*denom)-(y[t]-mu.p[t])^2/(2*denom^2))*partial.p.sg[t]
    partial.y.s[t] <- (y[t]-mu.p[t])/denom*partial.p.ms[t] - (1/(2*denom)-(y[t]-mu.p[t])^2/(2*denom^2))*(1+partial.p.ss[t])
  }
  partial.g <- sum(partial.y.g)
  partial.s <- sum(partial.y.s)
  return (list(partial.g=partial.g,partial.s = partial.s))
}
  
```

We check if the MLE estimates in approach 1 has small partial derivatives:

```{r}
partialDerivatives(y[1:80],mle$gamma,mle$Sigmaw)
```
This verifies the theory: we have the MLE estimates with partial derivatives with respect to $\gamma$ very close to 1. The partial derivatives with respect to $\sigma_w^2$, however, not quite well. This is likely that either our derivation with respect to $\sigma_w^2$ is wrong, or implementation is not correct due to tedious calculations.
In principle we should see both partial derivatives close to 1.

We will use the `optim` for our method as the naive implementation of the partial derivatives recursion is slower compared to built-in `optim`, and it is not quite correct.

# Read Data Analysis

We will set the first $T = $ for training to fit the MLE estimates of $\gamma$ and $\sigma_w^2$, then fit the whole model using `kf.gp`.

https://machinelearningmastery.com/time-series-data-stationary-python/

```{r}
read.file <- read.csv("female_birth.csv") %>% data.table()
dd1 <- read.file %>% select("Births") %>% mutate(t=1:nrow(read.file))
colnames(dd1)[1] <- "value"

plot(dd1$t, dd1$value)

acf(dd1$value)
```

# Comparison of Performance between KF and Gaussian Process Regression





